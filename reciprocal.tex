%
% Copyright © 2020 Peeter Joot.  All Rights Reserved.
% Licenced as described in the file LICENSE under the root directory of this GIT repository.
%
%{
\input{../latex/blogpost.tex}
\renewcommand{\basename}{reciprocal}
%\renewcommand{\dirname}{notes/phy1520/}
\renewcommand{\dirname}{notes/ece1228-electromagnetic-theory/}
%\newcommand{\dateintitle}{}
%\newcommand{\keywords}{}

\input{../latex/peeter_prologue_print2.tex}

\usepackage{amsthm}
\usepackage{peeters_layout_exercise}
\usepackage{peeters_braket}
\usepackage{peeters_figures}
\usepackage{siunitx}
\usepackage{verbatim}
%\usepackage{mhchem} % \ce{}
%\usepackage{macros_bm} % \bcM
%\usepackage{macros_qed} % \qedmarker
%\usepackage{txfonts} % \ointclockwise

\beginArtNoToc

\generatetitle{Reciprocal frame and curvilinear coordinates.}
%\chapter{Reciprocal frame uniqueness}
%\label{chap:reciprocal}

\section{Motivation.}
When studying curvilinear coordinates, reciprocal frame vectors are found to be the gradients of the parameters generating the tangent space at the point of evaluation.  We can also construct reciprocal frame vectors algebraically, but have a uniqueness doing so unless additional constraints are imposed.
In particular, the coordinates of a vector can be computed with respect to either the tangent space basis, or it's reciprocal basis, so we must impose the constraint that the reciprocal frame is restricted to the span of the tangent space basis.  Failing to impose such a constraint leads to uniqueness problems, and almost certainly means that the algebraic construction does not match the gradient construction.

In these notes, I'll walk through all the ideas involved.  The focus will be on the Dirac's algebra of special relativity, known as STA (Space Time Algebra) in geometric algebra parlance.

\paragraph{On notation.}
In Euclidean space we use bold face reciprocal frame vectors \( \Bx^i \cdot \Bx_j = {\delta^i}_j \), which nicely distinguishes them from the generalized coordinates \( x_i, x^j \) associated with the basis or the reciprocal frame, that is
\begin{equation}\label{eqn:reciprocal:640}
\Bx = x^i \Bx_i = x_j \Bx^j.
\end{equation}
On the other hand, it is conventional to use non-bold face for both the four-vectors and their coordinates in STA, such as the following standard basis decomposition
\begin{equation}\label{eqn:reciprocal:660}
x = x^\mu \gamma_\mu = x_\mu \gamma^\mu.
\end{equation}
If we use non-bold face \( x^\mu, x_\nu \) for the coordinates with respect to a specified frame, then we cannot also use non-bold face for the curvilinear basis vectors.
%One way to solve this notational problem would be to choose some new symbol, say \( f^\mu, f_\nu \), for our curvilinear basis vectors.
%This seems like a grossly arbitrary choice.  Symbols that are generally available without some additional or conflicting meaning can be hard to come by, so I'll opt to
To resolve this notational ambiguity, I've chosen to use bold face \( \Bx^\mu, \Bx_\nu \) symbols as the curvilinear basis elements in this relativistic context, as we do for Euclidean spaces.
\section{Basis and coordinates.}
\makedefinition{Standard Dirac basis.}{dfn:reciprocal:720}{
The Dirac basis elements are \(\setlr{ \gamma_0, \gamma_1, \gamma_2, \gamma_3 } \), satisfying
\begin{equation*}
\gamma_0^2 = 1 = -\gamma_k^2, \quad \forall k = 1,2,3,
\end{equation*}
and
\begin{equation}\label{eqn:reciprocal:740}
\gamma_\mu \cdot \gamma_\nu = 0, \quad \forall \mu \ne \nu.
\end{equation}
} % definition
A conventional way of summarizing these orthogonality relationships is \( \gamma_\mu \cdot \gamma_\nu = \eta_{\mu\nu} \), where
\( \eta_{\mu\nu} \) are the elements of the metric \( G = \diag(+,-,-,-) \).
\makedefinition{Reciprocal basis for the standard Dirac basis.}{dfn:reciprocal:760}{
We define a reciprocal basis \( \setlr{ \gamma^0, \gamma^1, \gamma^2, \gamma^3} \) satisfying \( \gamma^\mu \cdot \gamma_\nu = {\delta^\mu}_\nu, \forall \mu,\nu \in 0,1,2,3 \).
} % definition
\maketheorem{Reciprocal basis uniqueness.}{thm:reciprocal:780}{
This reciprocal basis is unique, and for our choice of metric has the values
\begin{equation*}
\gamma^0 = \gamma_0, \quad \gamma^k = -\gamma_k, \quad \forall k = 1,2,3.
\end{equation*}
} % theorem
Proof is left to the reader.
\makedefinition{Coordinates.}{dfn:reciprocal:800}{
We define the coordinates of a vector with respect to the standard basis as \( x^\mu \) satisfying
\begin{equation*}
x = x^\mu \gamma_\mu,
\end{equation*}
and define the coordinates of a vector with respect to the reciprocal basis as \( x_\mu \) satisfying
\begin{equation*}
x = x_\mu \gamma^\mu,
\end{equation*}
} % definition
\maketheorem{Coordinates.}{thm:reciprocal:820}{
Given the definitions above, we may compute the coordinates of a vector, simply by dotting with the basis elements
\begin{equation*}
x^\mu = x \cdot \gamma^\mu,
\end{equation*}
and
\begin{equation*}
x_\mu = x \cdot \gamma_\mu,
\end{equation*}
} % theorem
\begin{proof}
This follows by straightforward computation
\begin{dmath}\label{eqn:reciprocal:840}
x \cdot \gamma^\mu
=
\lr{ x^\nu \gamma_\nu } \cdot \gamma^\mu
=
x^\nu \lr{ \gamma_\nu \cdot \gamma^\mu }
=
x^\nu {\delta_\nu}^\mu
=
x^\mu,
\end{dmath}
and
\begin{dmath}\label{eqn:reciprocal:860}
x \cdot \gamma_\mu
=
\lr{ x_\nu \gamma^\nu } \cdot \gamma_\mu
=
x_\nu \lr{ \gamma^\nu \cdot \gamma_\mu }
=
x_\nu {\delta^\nu}_\mu
=
x_\mu.
\end{dmath}
\end{proof}
\section{Derivative operators.}
We'd like to determine the form of the (spacetime) gradient operator.  The gradient can be defined in terms of coordinates directly, but we choose an implicit definition, in terms of the directional derivative.
\makedefinition{Directional derivative and gradient.}{dfn:reciprocal:880}{
Let \( F = F(x) \) be a four-vector parameterized multivector.  The directional derivative of \( F \) with respect to the (four-vector) direction \( a \) is denoted
\begin{equation*}
\lr{ a \cdot \grad } F = \lim_{\epsilon \rightarrow 0} \frac{ F(x + \epsilon a) - F(x) }{ \epsilon },
\end{equation*}
where \( \grad \) is called the space time gradient.
} % definition
\maketheorem{Gradient.}{thm:reciprocal:900}{
The standard basis representation of the gradient is
\begin{equation*}
\grad = \gamma^\mu \partial_\mu,
\end{equation*}
where
\begin{equation*}
\partial_\mu = \PD{x^\mu}{}.
\end{equation*}
} % theorem
\begin{proof}
The Dirac gradient pops naturually out of the coordinate representation of the directional derivative, as we can see by
expanding \( F(x + \epsilon a) \) in Taylor series
\begin{dmath}\label{eqn:reciprocal:900}
F(x + \epsilon a)
= F(x) + \epsilon \frac{dF(x + \epsilon a)}{d\epsilon} + O(\epsilon^2)
= F(x) + \epsilon \PD{\lr{x^\mu + \epsilon a^\mu}}{F} \PD{\epsilon}{\lr{x^\mu + \epsilon a^\mu}}
= F(x) + \epsilon \PD{\lr{x^\mu + \epsilon a^\mu}}{F} a^\mu.
\end{dmath}
The directional derivative is
\begin{dmath}\label{eqn:reciprocal:920}
\lim_{\epsilon \rightarrow 0}
\frac{F(x + \epsilon a) - F(x)}{\epsilon}
=
\lim_{\epsilon \rightarrow 0}\,
a^\mu
\PD{\lr{x^\mu + \epsilon a^\mu}}{F}
=
a^\mu
\PD{x^\mu}{F}
=
\lr{a^\nu \gamma_\nu} \cdot \gamma^\mu \PD{x^\mu}{F}
=
a \cdot \lr{ \gamma^\mu \partial_\mu } F.
\end{dmath}
\end{proof}
\section{Curvilinear bases.}
Curvilinear bases are the foundation of the fundamental theorem of multivector calculus.  This form of integral calculus is defined over parameterized surfaces (called manifolds) that satisfy some specific non-degeneracy and continuity requirements.

A parameterized vector \( x(u,v, \cdots w) \) can be thought of as tracing out a hypersurface (curve, surface, volume, ...), where the dimension of the hypersurface depends on the number of parameters.
At each point, a bases can be constructed from the differentials of the parameterized vector.  Such a basis is called the tangent space to the surface at the point in question.  Our curvilinear bases will be related to these differentials.  We will also be interested in a dual basis that is restricted to the span of the tangent space.  This dual basis will be called the reciprocal frame, and line the basis of the tangent space itself, also varies from point to point on the surface.

One and two parameter spaces are illustrated in \cref{fig:tangentSpaceSurface:tangentSpaceSurfaceFig1}.
%\cref{fig:oneParameterDifferentialArrows:oneParameterDifferentialArrowsFig1}.
%\imageFigure{../figures/GAelectrodynamics/bw/oneParameterDifferentialArrowsFig1}{One parameter curve, and some tangent vectors.}{fig:oneParameterDifferentialArrows:oneParameterDifferentialArrowsFig1}{0.2}
%\cref{fig:tangentSpaceSurface:tangentSpaceSurfaceFig1}.
%\imageFigure{../figures/GAelectrodynamics/bw/tangentSpaceSurfaceFig1}{Two parameter curve, with some tangent planes.}{fig:tangentSpaceSurface:tangentSpaceSurfaceFig1}{0.2}
\imageTwoFigures
{../figures/GAelectrodynamics/bw/oneParameterDifferentialArrowsFig1}
{../figures/GAelectrodynamics/bw/tangentSpaceSurfaceFig1}{One and two parameter curves, with illustration of tangent spaces.}
{fig:tangentSpaceSurface:tangentSpaceSurfaceFig1}
{scale=0.6}

The tangent space basis at a specific point of a two parameter
surface, \( x(u^0, u^1) \),
is illustrated in
\cref{fig:twoParameterDifferentialCov:twoParameterDifferentialCovFig1}.  The differential directions that span the tangent space are
\begin{equation}\label{eqn:reciprocal:1040}
\begin{aligned}
d\Bx_0 &= \PD{u^0}{x} du^0 \\
d\Bx_1 &= \PD{u^1}{x} du^1,
\end{aligned}
\end{equation}
and the tangent space itself is \( \Span\setlr{ d\Bx_0, d\Bx_1 } \).  We may form an oriented surface area element \( d\Bx_0 \wedge d\Bx_1 \) over this surface.
\imageFigure{../figures/GAelectrodynamics/twoParameterDifferentialCovFig1}{Two parameter surface.}{fig:twoParameterDifferentialCov:twoParameterDifferentialCovFig1}{0.3}
Tangent spaces associated with 3 or more parameters cannot be easily visualized in three dimensions, but the idea generalizes algebraically without trouble.
\makedefinition{Tangent basis and space.}{dfn:reciprocal:100}{
Given a parameterization \( x = x(u^0, \cdots, u^N) \), where \( N < 4 \), the span of the vectors
\begin{equation*}
\Bx_\mu = \PD{u^\mu}{x},
\end{equation*}
is called the tangent space for the hypersurface associated with the parameterization, and it's basis is
\( \setlr{ \Bx_\mu } \).
%This subspace is called a manifold.
} % definition
Later we will see that parameterization constraints must be imposed, as not all surfaces generated by a set of parameterizations are useful for integration theory.  In particular, degenerate parameterizations for which the wedge products of the tangent space basis vectors are zero, or those wedge products cannot be inverted, are physically meaningful.  Properly behaved surfaces of this sort are called manifolds.

Having introduced curvilinear coordinates associated with a parameterization, we can now determine the form of the gradient with respect to a parameterization of spacetime.
\maketheorem{Gradient, curvilinear representation.}{thm:reciprocal:940}{
Given a spacetime parameterization \( x = x(u^0, u^1, u^2, u^3) \), the gradient with respect to the parameters \( u^\mu \) is
\begin{equation*}
\grad = \sum_\mu \Bx^\mu
	\PD{u^\mu}{},
\end{equation*}
where
\begin{equation*}
\Bx^\mu = \grad u^\mu.
\end{equation*}
The vectors \( \Bx^\mu \) are called the reciprocal frame vectors, and the
ordered set \( \setlr{ \Bx^0, \Bx^1, \Bx^2, \Bx^3 } \) is called the reciprocal basis.

It is convienient to define \( \partial_\mu \equiv \PDi{u^\mu}{} \), so that the gradient can be expressed in mixed index representation
\begin{equation*}
\grad = \Bx^\mu \partial_\mu.
\end{equation*}
This introduces some notational ambiguity, since we used \( \partial_\mu = \PDi{x^\mu}{} \) for the standard basis derivative operators too, but we will be careful to be explicit when there is any doubt about what is intended.
} % theorem
\begin{proof}
The proof follows by application of the chain rule.
\begin{dmath}\label{eqn:reciprocal:960}
\grad F
=
\gamma^\alpha \PD{x^\alpha}{F}
=
\gamma^\alpha
\PD{x^\alpha}{u^\mu}
\PD{u^\mu}{F}
=
\lr{ \grad u^\mu } \PD{u^\mu}{F}
=
\Bx^\mu \PD{u^\mu}{F}.
\end{dmath}
\end{proof}
\maketheorem{Reciprocal relationship.}{thm:reciprocal:120}{
The vectors \( \Bx^\mu = \grad u^\mu \), and \( \Bx_\mu = \PDi{u^\mu}{x} \) satisfy the reciprocal relationship
\begin{equation*}
   \Bx^\mu \cdot \Bx_\nu = {\delta^\mu}_\nu.
\end{equation*}
} % theorem
\begin{proof}
\begin{dmath}\label{eqn:reciprocal:1020}
   \Bx^\mu \cdot \Bx_\nu
   =
   \grad u^\mu \cdot
   \PD{u^\nu}{x}
   =
   \lr{
      \gamma^\alpha \PD{x^\alpha}{u^\mu}
   }
   \cdot
   \lr{
      \PD{u^\nu}{x^\beta} \gamma_\beta
   }
   =
   {\delta^\alpha}_\beta \PD{x^\alpha}{u^\mu}
      \PD{u^\nu}{x^\beta}
   =
   \PD{x^\alpha}{u^\mu} \PD{u^\nu}{x^\alpha}
   =
   \PD{u^\nu}{u^\mu}
   =
   {\delta^\mu}_\nu
.
\end{dmath}
\end{proof}
It is instructive to consider an example.  Here is a parameterization that scales the proper time parameter, and uses polar coordinates in the \(x-y\) plane.
\makeproblem{Compute the curivilinear and reciprocal basis.}{problem:reciprocal:1200}{
Given \( x(t,\rho,\theta,z) = c t \gamma_0 + \gamma_1 \rho e^{i \theta} + z \gamma_3 \), where \( i = \gamma_1 \gamma_2 \), compute the curvilinear frame vectors and their reciprocals.
} % problem
\makeanswer{problem:reciprocal:1200}{
The frame vectors are all easy to compute
\begin{equation}\label{eqn:reciprocal:1180}
\begin{aligned}
\Bx_0 &= \PD{t}{x} = c \gamma_0 \\
\Bx_1 &= \PD{\rho}{x} = \gamma_1 e^{i \theta} \\
\Bx_2 &= \PD{\theta}{x} = \rho \gamma_1 \gamma_1 \gamma_2 e^{i \theta} = - \rho \gamma_2 e^{i \theta} \\
\Bx_3 &= \PD{z}{x} = \gamma_3.
\end{aligned}
\end{equation}
The \( \Bx_1 \) vector is radial, \( \Bx^2 \) is perpendicular to that tangent to the same unit circle, as plotted in
\cref{fig:rotationCurvilinear:rotationCurvilinearFig1}.
\imageFigure{../figures/classicalmechanics/rotationCurvilinearFig1}{Tangent space direction vectors.}{fig:rotationCurvilinear:rotationCurvilinearFig1}{0.3}
All of these particular frame vectors happen to be mutually perpendicular, something that will not generally be true for a more arbitrary parameterization.

To compute the reciprocal frame vectors, we must express our parameters in terms of \( x^\mu \) coordinates, and use implicit integration techniques to deal with the coupling of the rotational terms.  First observe that
\begin{dmath}\label{eqn:reciprocal:1200}
\gamma_1 e^{i\theta}
= \gamma_1 \lr{ \cos\theta + \gamma_1 \gamma_2 \sin\theta }
= \gamma_1 \cos\theta - \gamma_2 \sin\theta,
\end{dmath}
so
\begin{equation}\label{eqn:reciprocal:1220}
\begin{aligned}
x^0 &= c t \\
x^1 &= \rho \cos\theta \\
x^2 &= -\rho \sin\theta \\
x^3 &= z.
\end{aligned}
\end{equation}
We can easily evaluate the \( t, z \) gradients
\begin{equation}\label{eqn:reciprocal:1240}
\begin{aligned}
\grad t &= \frac{\gamma^1 }{c} \\
\grad z &= \gamma^3,
\end{aligned}
\end{equation}
but the \( \rho, \theta \) gradients are not as easy.  First writing
\begin{dmath}\label{eqn:reciprocal:1260}
\rho^2 = \lr{x^1}^2 + \lr{x^2}^2,
\end{dmath}
we find
\begin{dmath}\label{eqn:reciprocal:1280}
2 \rho \grad \rho = 2 \lr{ x^1 \grad x^1 + x^2 \grad x^2 }
= 2 \rho \lr{ \cos\theta \gamma^1 - \sin\theta \gamma^2 }
= 2 \rho \gamma^1 \lr{ \cos\theta - \gamma_1 \gamma^2 \sin\theta }
= 2 \rho \gamma^1 e^{i\theta},
\end{dmath}
so
\begin{dmath}\label{eqn:reciprocal:1300}
\grad \rho = \gamma^1 e^{i\theta}.
\end{dmath}
For the \( \theta \) gradient, we can write
\begin{dmath}\label{eqn:reciprocal:1320}
\tan\theta = -\frac{x^2}{x^1},
\end{dmath}
so
\begin{dmath}\label{eqn:reciprocal:1340}
\inv{\cos^2 \theta} \grad \theta
= -\frac{\gamma^2}{x^1} - x^2 \frac{-\gamma^1}{\lr{x^1}^2}
= \inv{\lr{x^1}^2} \lr{ - \gamma^2 x^1 + \gamma^1 x^2 }
= \frac{\rho}{\rho^2 \cos^2\theta } \lr{ - \gamma^2 \cos\theta - \gamma^1 \sin\theta }
= -\frac{1}{\rho \cos^2\theta } \gamma^2 \lr{ \cos\theta + \gamma_2 \gamma^1 \sin\theta }
= -\frac{\gamma^2 e^{i\theta} }{\rho \cos^2\theta },
\end{dmath}
or
\begin{dmath}\label{eqn:reciprocal:1360}
\grad\theta = -\inv{\rho} \gamma^2 e^{i\theta}.
\end{dmath}
In summary,
\begin{equation}\label{eqn:reciprocal:1380}
\begin{aligned}
\Bx^0 &= \frac{\gamma^0}{c} \\
\Bx^1 &= \gamma^1 e^{i\theta} \\
\Bx^2 &= -\inv{\rho} \gamma^2 e^{i\theta} \\
\Bx^3 &= \gamma^3.
\end{aligned}
\end{equation}
} % answer
Despite being a fairly simple parameterization, it was still fairly difficult to solve for the gradients when the parameterization introduced coupling between the coordinates.  In this particular case, we could have solved for the parameters in terms of the coordinates (but it was easilier not to), but that will not generally be true.  We want a less labour intensive strategy to find the reciprocal frame.  When we have a full parameterization of spacetime, then we can do this with nothing more than a matrix inversion.
\maketheorem{Reciprocal frame matrix equations.}{thm:reciprocal:140}{
Given a spacetime basis \( \setlr{\Bx_0, \cdots \Bx_3} \), let \( [\Bx_\mu] \) and \( [\Bx^\nu] \) be column matrices with the coordinates of these vectors and their reciprocals, with respect to the standard basis \( \setlr{\gamma_0, \gamma_1, \gamma_2, \gamma_3 } \).
Let
\begin{equation*}
A =
\begin{bmatrix}
   [\Bx_0] & \cdots & [\Bx_{3}]
\end{bmatrix}
,\qquad
X =
\begin{bmatrix}
   [\Bx^0] & \cdots & [\Bx^{3}]
\end{bmatrix}.
\end{equation*}
The coordinates of the reciprocal frame vectors can be found by solving
\begin{equation*}
   A^\T G X = 1,
\end{equation*}
where \( G = \diag(1,-1,-1,-1) \) and the RHS is an \( 4 \times 4 \) identity matrix.
} % theorem
%In the special case that \( X \) contains zero rows, then they can be removed (with corresponding adjustments to \( G, B \)), and the reduced equation, which may be square, can be solved instead.  Such a reduced set of equations may provide the reciprocal frame vectors that satisfy the gradient identity \cref{eqn:reciprocal:260}, but that really needs to be shown.
\begin{proof}
Let \( \Bx_\mu = {a_\mu}^\alpha \gamma_\alpha, \Bx^\nu = b^{\nu\beta} \gamma_\beta \), so that
\begin{dmath}\label{eqn:reciprocal:140}
   A =
\begin{bmatrix}
   {a_\nu}^\mu
\end{bmatrix},
\end{dmath}
and
\begin{dmath}\label{eqn:reciprocal:160}
   X =
\begin{bmatrix}
   b^{\nu\mu}
\end{bmatrix},
\end{dmath}
where \( \mu \in [0,3]\) are the row indexes and \( \nu \in [0,N-1]\) are the column indexes.
The reciprocal frame satisfies \( \Bx_\mu \cdot \Bx^\nu = {\delta_\mu}^\nu \), which has the coordinate representation of
\begin{dmath}\label{eqn:reciprocal:180}
\Bx_\mu \cdot \Bx^\nu
=
\lr{
   {a_\mu}^\alpha \gamma_\alpha
}
\cdot
\lr{
   b^{\nu\beta} \gamma_\beta
}
=
   {a_\mu}^\alpha
   \eta_{\alpha\beta}
   b^{\nu\beta}
=
{[A^\T G B]_\mu}^\nu,
\end{dmath}
where \( \mu \) is the row index and \( \nu \) is the column index.
\end{proof}
\makeproblem{Matrix inversion reciprocals.}{problem:reciprocal:1400}{
For the parameterization of \cref{problem:reciprocal:1200}, find the reciprocal frame vectors by matrix inversion.
} % problem
\makeanswer{problem:reciprocal:1400}{
We expanded \( \Bx_1 \) explicitly in \cref{eqn:reciprocal:1200}.  Doing the same for \( \Bx_2 \), we have
\begin{dmath}\label{eqn:reciprocal:1201}
\Bx_2 =
-\rho \gamma_2 e^{i\theta}
= -\rho \gamma_2 \lr{ \cos\theta + \gamma_1 \gamma_2 \sin\theta }
= - \rho \lr{ \gamma_2 \cos\theta + \gamma_1 \sin\theta}.
\end{dmath}
Reading off the coordinates of our frame vectors, we have
\begin{dmath}\label{eqn:reciprocal:1400}
X =
\begin{bmatrix}
c &  0 & 0       & 0 \\
0 &  C & -\rho S & 0 \\
0 & -S & -\rho C & 0 \\
0 &  0 & 0       & 1 \\
\end{bmatrix}
\end{dmath},
where \( C = \cos\theta \) and \( S = \sin\theta \).
we want
\begin{equation}\label{eqn:reciprocal:1420}
Y =
{\begin{bmatrix}
c &  0      & 0       &  0 \\
0 & -C      &  S      &  0 \\
0 &  \rho S &  \rho C &  0 \\
0 &  0      & 0       & -1 \\
\end{bmatrix}}^{-1}
=
\begin{bmatrix}
\inv{c}   &  0 & 0               &  0 \\
        0 & -C & \frac{S}{\rho}  &  0 \\
        0 &  S & \frac{C}{\rho}  &  0 \\
        0 &  0 & 0               & -1 \\
\end{bmatrix}.
\end{equation}
We can read off the coordinates of the reciprocal frame vectors
\begin{equation}\label{eqn:reciprocal:1440}
\begin{aligned}
\Bx^0 &= \inv{c} \gamma_0 \\
\Bx^1 &= -\cos\theta \gamma_1 + \sin\theta \gamma_2 \\
\Bx^2 &= \inv{\rho} \lr{ \sin\theta \gamma_1 + \cos\theta \gamma_2 } \\
\Bx^3 &= -\gamma_3 \\
\end{aligned}
\end{equation}
Factoring out \( \gamma^1 \) from the \( \Bx^1 \) terms, we find
\begin{dmath}\label{eqn:reciprocal:1460}
\Bx^1
= -\cos\theta \gamma_1 + \sin\theta \gamma_2
= \gamma^1 \lr{ \cos\theta + \gamma_1 \gamma_2 \sin\theta }
= \gamma^1 e^{i\theta}.
\end{dmath}
Similarly for \( \Bx^2 \),
\begin{dmath}\label{eqn:reciprocal:1480}
\Bx^2
= \inv{\rho} \lr{ \sin\theta \gamma_1 + \cos\theta \gamma_2 }
= \frac{\gamma^2}{\rho} \lr{ \sin\theta \gamma_2 \gamma_1 - \cos\theta }
= -\frac{\gamma^2}{\rho} e^{i\theta}.
\end{dmath}
This matches \cref{eqn:reciprocal:1380}, as expected, but required only algebraic work to compute.
} % answer
There will be circumstances where we parameterize only a subset of spacetime, and are interested in calculating quantities associated with such a surface.  For example, suppose that
\begin{dmath}\label{eqn:reciprocal:1500}
x(\rho,\theta) = \gamma_1 \rho e^{i \theta},
\end{dmath}
where \( i = \gamma_1 \gamma_2 \) as before.  We are now parameterizing only the \(x-y\) plane.  We will still find
\begin{equation}\label{eqn:reciprocal:1520}
\begin{aligned}
\Bx_1 &= \gamma_1 e^{i \theta} \\
\Bx_2 &= -\gamma_2 \rho e^{i \theta}.
\end{aligned}
\end{equation}
We can compute the reciprocals of these vectors using the gradient method.  It's possible to state matrix equations representing the reciprocal relationship of \cref{thm:reciprocal:120}, which, in this case, is \( X^\T G Y = 1 \), where the RHS is a \( 2 \times 2 \) identity matrix, and \( X, Y\) are \( 4\times 2\) matrices of coordinates, with
\begin{equation}\label{eqn:reciprocal:1540}
X =
\begin{bmatrix}
  0 & 0        \\
  C & -\rho S  \\
 -S & -\rho C  \\
  0 & 0
\end{bmatrix}.
\end{equation}
We no longer have a square matrix problem to solve, and our solution set is multivalued.
In particular, this matrix equation has solutions
\begin{equation}\label{eqn:reciprocal:1460}
\begin{aligned}
\Bx^1 &= \gamma^1 e^{i\theta} + \alpha \gamma^0 + \beta \gamma^3 \\
\Bx^2 &= -\frac{\gamma^2}{\rho} e^{i\theta} + \alpha' \gamma^0 + \beta' \gamma^3.
\end{aligned}
\end{equation}
where \( \alpha, \alpha', \beta, \beta' \) are arbitrary constants.
In the example we considered, we saw that our \( \rho, \theta \) parameters were functions of only \( x^1, x^2 \), so taking gradients could not introduce any \( \gamma^0, \gamma^3 \) dependence in \( \Bx^1, \Bx^2 \).
It seems reasonable to assert that we seek an
algebraic method of computing a set of vectors that satisfies the reciprocal relationships, where that set of vectors is restricted to the tangent space.  We will need to figure out how to prove that this reciprocal construction is identical to the parameter gradients, but let's start with figuring out what such a tangent space restricted solution looks like.
\makeproblem{Reciprocal frame for two parameter subspace.}{problem:reciprocal:1500}{
Let \( \Bx_1, \Bx_2 \) be two vectors.  Find \( \Bx^1, \Bx^2 \in \Span\setlr{ \Bx_1, \Bx_2 } \) such that \( \Bx^\mu \cdot \Bx_\nu = {\delta^\mu}_\nu \).
} % problem
\makeanswer{problem:reciprocal:1500}{
} % answer

\chapter{INTEGRATION STUFF FOR LATER.}
\makedefinition{Hypervolume elements.}{dfn:reciprocal:1060}{
Wedge products of the differentials that lie along each of the tangent basis directions
\begin{equation*}
d\Bx_\mu = \Bx_\mu du^\mu, \qquad \mbox{(no sum)},
\end{equation*}
can be used to form line, surface, volume, and hypervolume elements
\begin{equation*}
\begin{aligned}
d^1\Bx &= d\Bx_0 = \Bx_0 du^0 \\
d^2\Bx &= d\Bx_0 \wedge d\Bx_1 = \lr{\Bx_0 \wedge \Bx_1} du^0 du^1 \\
d^3\Bx &= d\Bx_0 \wedge d\Bx_1 \wedge d\Bx_2 = \lr{\Bx_0 \wedge \Bx_1 \wedge \Bx_2 } du^0 du^1 du^2 \\
d^4\Bx &= d\Bx_0 \wedge d\Bx_1 \wedge d\Bx_2 \wedge d\Bx_3 = \lr{\Bx_0 \wedge \Bx_1 \wedge \Bx_2 \wedge \Bx_3 } du^0 du^1 du^2 du^3.
\end{aligned}
\end{equation*}
%will be of interest when formulating the fundamental theorem of geometric algebra.
} % definition
For such hypervolume elements to be meaningful, we must impose a non-degeneracy constraint, and require that the
%In particular, the space must be non-degenerate, which means that the
volume elements above are non-zero (the Jacobian's of the partials are not zero.)  This non-zero constraint is required to ensure that the orientation of the volume elements do not change sign (as in differential forms, we do not take the absolute value of any Jacobians.)
We must also require that these volume elements are invertible, since non-invertible null vectors, bivectors, and trivectors,
are possible in our non-Euclidean space.
The parameterized hypersurfaces that we integrate over, when augmented with these additional constraints, are referred to as manifolds.

Consider the following example of a degenerate parameterization.
\makeproblem{Degenerate surface parameterization.}{problem:reciprocal:480}{
Given a spacetime plane parameterization \( x(u,v) = u a + v b \), where
\begin{dmath}\label{eqn:reciprocal:480}
	a = \gamma_0 + \gamma_1 + \gamma_2 + \gamma_3,
\end{dmath}
\begin{dmath}\label{eqn:reciprocal:500}
	b = \gamma_0 - \gamma_1 + \gamma_2 - \gamma_3,
\end{dmath}
show that this is a degenerate parameterization, and find the bivector that represents the tangent space.
Are these vectors lightlike, spacelike, or timelike?  Comment on whether this parameterization represents a physically relevant spacetime surface.
} % problem
\makeanswer{problem:reciprocal:480}{
To characterize the vectors, we square them
\begin{equation}\label{eqn:reciprocal:1080}
a^2 = b^2 =
\gamma_0^2 +
\gamma_1^2 +
\gamma_2^2 +
\gamma_3^2
=
1 - 3
= -2,
\end{equation}
so \( a, b \) are both spacelike vectors.  The tangent space is clearly just \( \Span \setlr{ a, b } = \Span \setlr{ e, f }\) where
\begin{equation}\label{eqn:reciprocal:1100}
\begin{aligned}
e &= \gamma_0 + \gamma_2 \\
f &= \gamma_1 + \gamma_3.
\end{aligned}
\end{equation}
Observe that \( a = e + f, b = e - f \), and that both \( e, f \) are lightlike (\( e^2 = f^2 = 0 \).)
The bivector for the tangent plane is
\begin{dmath}\label{eqn:reciprocal:1120}
\gpgradetwo{
a b
}
=
\gpgradetwo{
(e + f) (e - f)
}
=
\gpgradetwo{
e^2 - f^2 - 2 e f
}
= -2 e f,
\end{dmath}
where
\begin{dmath}\label{eqn:reciprocal:1140}
e f = \gamma_{01} + \gamma_{21} + \gamma_{23} + \gamma_{03}.
\end{dmath}
Because \( e, f \) are both lightlike (zero square), the bivector \( e f \) also squares to zero (compute this if in doubt),
which shows that the parameterization is degenerate.

Despite neither \( a, b \) being lightlike, the parameterization can also be expressed as
\begin{dmath}\label{eqn:reciprocal:1160}
x(u,v)
= u ( e + f ) + v ( e - f )
= (u + v) e + (u - v) f,
\end{dmath}
a linear combination of a pair of lightlike vectors.  This is clearly not a physically meaningful spacetime surface.
} % answer

\EndArticle
%\EndNoBibArticle
