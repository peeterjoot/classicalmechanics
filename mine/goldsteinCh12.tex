%
% Copyright © 2012 Peeter Joot.  All Rights Reserved.
% Licenced as described in the file LICENSE under the root directory of this GIT repository.
%

\label{chap:goldsteinCh12}

Now have the so often cited \citep{goldstein1951cm} book to study (an ancient version from the 50's).  Here is an attempt at a few of the problems.  Some problems were tackled but omitted here since they overlapped with those written up in \bookchapcite{PJTongMf1}{phy354} before getting this book.

\makeproblem{Kinetic energy for barbell shaped object (1.6)}{gold:ch1:pr6}{
Two points of mass \(m\) are joined by a rigid weightless rod of length \(l\), the center of which is constrained to move on a circle of radius \(a\).  Set up the kinetic energy in generalized coordinates.
}
\makeanswer{gold:ch1:pr6}{

Barbell shape, equal masses.  center of rod between masses constrained to circular motion.

Assuming motion in a plane, the equation for the center of the rod is:

\begin{equation*}
c = a e^{i\theta}
\end{equation*}

and the two mass points positions are:
\begin{equation}\label{eqn:goldsteinCh12:20}
\begin{aligned}
q_1 &= c + (l/2) e^{i\alpha} \\
q_2 &= c - (l/2) e^{i\alpha}
\end{aligned}
\end{equation}

taking derivatives:
\begin{equation}\label{eqn:goldsteinCh12:40}
\begin{aligned}
\qdot_1 &= a i \dottheta e^{i\theta} + (l/2) i \dotalpha e^{i\alpha} \\
\qdot_2 &= a i \dottheta e^{i\theta} - (l/2) i \dotalpha e^{i\alpha} \\
\end{aligned}
\end{equation}

and squared magnitudes:

\begin{equation}\label{eqn:goldsteinCh12:60}
\begin{aligned}
\qdot_{\pm}
&= \Abs{a \dottheta \pm (l/2) \dotalpha e^{i(\alpha - \theta)}}^2 \\
&= \left(a \dottheta   \pm   \inv{2} l \dotalpha \cos(\alpha - \theta)\right)^2 + \left(\inv{2} l \dotalpha \sin(\alpha - \theta)\right)^2
\end{aligned}
\end{equation}

Summing the kinetic terms yields

\begin{equation*}
K = m \left(a \dottheta \right)^2 + m \left(\inv{2} l \dotalpha\right)^2
\end{equation*}

Summing the potential energies, presuming that the motion is vertical, we have:

\begin{equation*}
V = m g (l/2) \cos\theta - m g (l/2) \cos \theta
\end{equation*}

So, the Lagrangian is just the Kinetic energy.

Taking derivatives to get the EOMs we have:

\begin{equation}\label{eqn:goldsteinCh12:80}
\begin{aligned}
\lr{ m a^2 \dottheta }' &= 0 \\
\left(\inv{4} m l^2 \dotalpha \right)' &= 0
\end{aligned}
\end{equation}

This is surprising seeming.  Is this correct?
}

\makeproblem{Angular momentum conservation of three particle system (1.8)}{gold:ch1:pr8}{
A system is composed of three particles of equal mass m.  Between any two of them there are forces derivable from a potential

\begin{equation*}
V = -g e^{-\mu r}
\end{equation*}

where r is the distance between the two particles.  In addition, two of the particles each exert a force on the third which can be obtained from a generalized potential of the form

\begin{equation*}
U = -f \Bv \cdot \Br
\end{equation*}

\(\Bv\) being the relative velocity of the interacting particles and f a constant.  Set up the Lagrangian for the system, using as coordinates the radius vector \(\BR\) of the center of mass and the two vectors

\begin{equation}\label{eqn:goldsteinCh12:100}
\begin{aligned}
\Brho_1 &= \Br_1 - \Br_3 \\
\Brho_2 &= \Br_2 - \Br_3
\end{aligned}
\end{equation}

Is the total angular momentum of the system conserved?
}

\makeanswer{gold:ch1:pr8}{
The center of mass vector is:

\begin{equation*}
\BR = \inv{3}(\Br_1 + \Br_2 + \Br_3)
\end{equation*}

This can be used to express each of the position vectors in terms of the \(\Brho_i\) vectors:

\begin{equation}\label{eqn:goldsteinCh12:120}
\begin{aligned}
3 m \BR &= m (\Brho_1 + \Br_3) + m(\Brho_2 + \Br_3) + m \Br_3 \\
        &= 2 m (\Brho_1 + \Brho_2) + 3 m \Br_3 \\
  \Br_3 &= \BR - \inv{3}(\Brho_1 + \Brho_2) \\
\Br_2 = \Brho_2 + \Br_3 &= \Brho_2 + \Br_3 = \frac{2}{3} \Brho_2 - \inv{2} \Brho_1 + \BR \\
\Br_1 = \Brho_1 + \Br_3 &= \frac{2}{3} \Brho_1 - \inv{2} \Brho_2 + \BR \\
\end{aligned}
\end{equation}

Now, that is enough to specify the part of the Lagrangian from the potentials that act between all the particles

\begin{equation*}
\LL_V = \sum -V_{ij} = g \left( e^{-\mu \Abs{\Brho_1}} + e^{-\mu \Abs{\Brho_2}} + e^{-\mu \Abs{ \Brho_1 - \Brho_2 }} \right)
\end{equation*}

Now, we need to calculate the two \(U\) potential terms.  If we consider with positions \(\Br_1\), and \(\Br_2\) to be the ones
that can exert a force on the third, the velocities of those masses relative to \(\Br_3\) are:

\begin{equation*}
(\Br_3 - \Br_k)' = \dot{\Brho_k}
\end{equation*}

So, the potential parts of the Lagrangian are
%Adding this to the first half of the Lagrangian we have:
%So, for the second half of the Lagrangian we have:

\begin{equation*}
\LL_{U+V} =
g \left( e^{-\mu \Abs{\Brho_1}} + e^{-\mu \Abs{\Brho_2}} + e^{-\mu \Abs{ \Brho_1 - \Brho_2 }} \right)
+ f \left(\BR - \inv{3}(\Brho_1 + \Brho_2) \right) \cdot \left( \dot{\Brho_1} + \dot{\Brho_2} \right)
\end{equation*}

The Kinetic part (omitting the m/2 factor), in terms of the CM and relative vectors is

\begin{equation}\label{eqn:goldsteinCh12:140}
\begin{aligned}
%\inv{2}m
%\left(
(\Bv_1)^2
+(\Bv_2)^2
+(\Bv_3)^2 %\right)
&= \left(\frac{2}{3} \dot{\Brho}_1 - \inv{2} \dot{\Brho}_2 + \dot{\BR}\right)^2 + \left(\frac{2}{3} \dot{\Brho}_2 - \inv{2} \dot{\Brho}_1 + \dot{\BR}\right)^2 + \left(\dot{\BR} - \inv{3}(\dot{\Brho}_1 + \dot{\Brho}_2)\right)^2 \\
&=
 3 \dot{\BR}^2 + (5/9 + 1/4) ((\dot{\Brho}_1)^2 + (\dot{\Brho}_2)^2 ) \\
&+ 2 (-2/3 + 1/9) \dot{\Brho}_1 \cdot \dot{\Brho}_1
+ 2 (1/3-1/2) (\dot{\Brho}_1 + \dot{\Brho}_2) \cdot \dot{\BR}  \\
\end{aligned}
\end{equation}

So the Kinetic part of the Lagrangian is

\begin{equation}\label{eqn:goldsteinCh12:160}
\begin{aligned}
\LL_K &= \frac{3m}{2} \dot{\BR}^2 + \frac{29 m}{72} ((\dot{\Brho}_1)^2 + (\dot{\Brho}_2)^2 )
- \frac{5 m}{9} \dot{\Brho}_1 \cdot \dot{\Brho}_2
- \frac{m}{6} (\dot{\Brho}_1 + \dot{\Brho}_2) \cdot \dot{\BR}  \\
\end{aligned}
\end{equation}

and finally, the total Lagrangian is

\begin{equation}\label{eqn:goldsteinCh12:180}
\begin{aligned}
\LL =
\frac{3m}{2} \dot{\BR}^2 + \frac{29 m}{72} ((\dot{\Brho}_1)^2 + (\dot{\Brho}_2)^2 )
- \frac{5 m}{9} \dot{\Brho}_1 \cdot \dot{\Brho}_2
- \frac{m}{6} (\dot{\Brho}_1 + \dot{\Brho}_2) \cdot \dot{\BR}  \\
+g \left( e^{-\mu \Abs{\Brho_1}} + e^{-\mu \Abs{\Brho_2}} + e^{-\mu \Abs{ \Brho_1 - \Brho_2 }} \right)
+ f \left(\BR - \inv{3}(\Brho_1 + \Brho_2) \right) \cdot \left( \dot{\Brho_1} + \dot{\Brho_2} \right)
\end{aligned}
\end{equation}

\paragraph{Angular momentum conservation?}

How about the angular momentum conservation question?  How to answer that?  One way would be to compute the forces from the Lagrangian, and take cross products but is that really the best way?  Perhaps the answer is as simple as observing that there are no external torque's on the system, thus \(d\BL/dt = 0\), or angular momentum for the system is constant (conserved).  Is that actually the case
with these velocity dependent potentials?

It was suggested to me on PF that I should look at how this Lagrangian transforms under rotation, and use Noether's theorem.
The Goldstein book does not explicitly mention this theorem that I can see, and I do not think it was covered yet if it did.

Suppose we did know about Noether's theorem for this problem (as I now do with
in this revisiting of this problem to complete it), we would have to
see if the Lagrangian is invariant under rotation.  Suppose that a rigid rotation is introduced, which we can write in GA
formalism using dual sided quaternion products

\begin{equation}\label{eqn:goldsteinCh12:200}
\begin{aligned}
\Bx \rightarrow \Bx' = e^{-i\ncap \alpha/2} \Bx e^{i\ncap\alpha/2}
\end{aligned}
\end{equation}

(could probably also use a matrix formulation, but the parametrization is messier).

For all the relative vectors \(\Brho_k\) we have
\begin{equation}\label{eqn:goldsteinCh12:220}
\Abs{\Brho_k'} = \Abs{\Brho_k}.
\end{equation}
So all the \(V\) potential interactions are invariant.

Since the rotation quaternion here is a fixed non-time dependent quantity we have
\begin{equation}\label{eqn:goldsteinCh12:240}
\dot{\Brho}_k' = e^{-i\ncap \alpha/2} \dot{\Brho}_k e^{i\ncap\alpha/2},
\end{equation}
so for the dot product in the remaining potential term we have
\begin{equation}\label{eqn:goldsteinCh12:260}
\begin{aligned}
\left( \BR' - \inv{3}\left(\Brho_1' + \Brho_2'\right) \right) \cdot \left({\dot{\Brho}}_1' + {\dot{\Brho}}_2'\right)
&=
\left(e^{-i\ncap \alpha/2} \left( \BR - \inv{3}\left(\Brho_1 + \Brho_2\right) \right) e^{i\ncap\alpha/2}\right) \cdot
\left(e^{-i\ncap \alpha/2} \dot{\Brho}_1 + \dot{\Brho_2} e^{i\ncap\alpha/2}\right) \\
&=
\gpgradezero{
e^{-i\ncap \alpha/2} \left( \BR - \inv{3}\left(\Brho_1 + \Brho_2\right) \right) e^{i\ncap\alpha/2}
e^{-i\ncap \alpha/2} \dot{\Brho}_1 + \dot{\Brho_2} e^{i\ncap\alpha/2}
} \\
&=
\gpgradezero{
e^{-i\ncap \alpha/2} \left( \BR - \inv{3}\left(\Brho_1 + \Brho_2\right) \right) \left(\dot{\Brho}_1 + \dot{\Brho_2}\right) e^{i\ncap\alpha/2}
} \\
&=
\gpgradezero{
e^{i\ncap\alpha/2}
e^{-i\ncap \alpha/2} \left( \BR - \inv{3}\left(\Brho_1 + \Brho_2\right) \right) \left(\dot{\Brho}_1 + \dot{\Brho_2}\right)
} \\
&=
\gpgradezero{
\left( \BR - \inv{3}\left(\Brho_1 + \Brho_2\right) \right) \left(\dot{\Brho}_1 + \dot{\Brho_2}\right)
} \\
&=
\left( \BR - \inv{3}\left(\Brho_1 + \Brho_2\right) \right) \cdot \left(\dot{\Brho}_1 + \dot{\Brho_2}\right)
\end{aligned}
\end{equation}

So, presuming I interpreted the \(\Br\) in \(\Bv \cdot \Br\) correctly,
all the vector quantities in the Lagrangian are rotation invariant, and by Noether's we should have system angular momentum
conservation.

\paragraph{Application of Noether's}
Invoking Noether's here seems like cheating, at least without computing the conserved current, so let us do this.
%  Should try something more direct here for comparision.  As a bare minimum the conserved current should
%also be computed (which should be the angular momentum).

%\subsection{Evaluate the equations of motion}

To make this easier, suppose we generalize the Lagrangian slightly to get rid of all the peculiar and specific numerical constants.  Let
\(\rho_3 = \BR\), then our Lagrangian has the functional form

\begin{equation}\label{eqn:goldsteinCh12:280}
\begin{aligned}
\LL = \alpha^{ij} \dot{\Brho}_i \cdot \dot{\Brho}_j
+ g^i e^{-\mu \Abs{\Brho_i}}
+ g^{ij} e^{-\mu \Abs{ \Brho_i - \Brho_j }}
+ f^i \Brho_i \cdot ( \dot{\Brho}_1 + \dot{\Brho}_2 )
\end{aligned}
\end{equation}

We can then pick specific \(\alpha^{ij}\), \(f^{i}\), and \(g^{ij}\) (not all non-zero), to match the Lagrangian of this problem.
This could be expanded in terms of coordinates, producing nine generalized coordinates and nine corresponding velocity terms, but
since our Lagrangian transformation is so naturally expressed in vector form this does not seem like a reasonable thing to do.

Let us step up the abstraction one more level instead and treat the Noether symmetry in the more general case, supposing that we have
a Lagrangian that is invariant under the same rotational transformation applied above, but has the following general form with explicit
vector parametrization, where as above, all our vectors come in functions of the dot products (either explicit or implied by
absolute values) of our vectors or their time derivatives

\begin{equation}\label{eqn:goldsteinCh12:300}
\begin{aligned}
\LL = f( {\Bx_k} \cdot {{\Bx}_j}, {\Bx_k} \cdot {\dot{\Bx}_j}, {\dot{\Bx}_k} \cdot {\dot{\Bx}_j} )
\end{aligned}
\end{equation}

Having all the parametrization being functions of dot products gives the desired rotational symmetry for the Lagrangian.
This must be however, not a dot product with an arbitrary vector, but one of the generalized vector parameters of the Lagrangian.
Something like the \(\BA \cdot \Bv\) term in the Lorentz force Lagrangian does not have this invariance since \(\BA\) does not transform
along with \(\Bv\).  Also Note that the absolute values of the \(\Brho_k\) vectors are functions of dot products.

Now we are in shape to compute the conserved ``current'' for a rotational symmetry.  Our vectors and their derivatives are explicitly rotated

\begin{equation}\label{eqn:goldsteinCh12:320}
\begin{aligned}
{\Bx}_k' &= e^{-i\ncap \alpha/2} {\Bx}_k e^{i\ncap\alpha/2} \\
\dot{\Bx}_k' &= e^{-i\ncap \alpha/2} \dot{\Bx}_k e^{i\ncap\alpha/2} \\
\end{aligned}
\end{equation}

and our Lagrangian is assumed, as above with all vectors coming in dot product pairs, to have rotational invariance when all the vectors
in the system are rotated

\begin{equation}\label{eqn:goldsteinCh12:340}
\begin{aligned}
\LL & \rightarrow \LL'(\Bx_k', \dot{\Bx}_j') = \LL(\Bx_k, \dot{\Bx}_j) \\
\end{aligned}
\end{equation}

The essence of Noether's theorem was applied chain rule, looking at how the transformed Lagrangian changes with respect to the transformation.  In this case
we want to calculate

\begin{equation}\label{eqn:goldsteinCh12:360}
\begin{aligned}
{\left. \frac{d \LL'}{d\alpha} \right\vert}_{\alpha=0}
\end{aligned}
\end{equation}

First seeing the Noether's derivation, I did not understand why the evaluation at \(\alpha=0\) was required, even after doing this
derivation for myself in \bookchapcite{PJEulerLagrange}{phy354} (after an initial botched attempt), but
the reason for it actually became clear with this application, as writing it up will show.

Anyways, back to the derivative.  One way to evaluate this would be in terms of coordinates, writing \(\Bx_k' = \Be^m x_{km}'\),

\begin{equation}\label{eqn:goldsteinCh12:380}
\begin{aligned}
\frac{d \LL'}{d\alpha} (\Bx_k', \dot{\Bx}_j')
&= \sum_{k,m} \frac{\partial \LL'}{\partial x_{km}'} \PD{\alpha}{x_{km}'} + \frac{\partial \LL'}{\partial \dot{x}_{km}'} \PD{\alpha}{\dot{x}_{km}'} \\
\end{aligned}
\end{equation}

This is a bit of a mess however, and begs for some shorthand.  Let us write

\begin{equation}\label{eqn:goldsteinCh12:400}
\begin{aligned}
\spacegrad_{\Bx_k'} \LL' &= e^m \frac{\partial \LL'}{\partial x_{km}'} \\
\spacegrad_{\dot{\Bx}_k'} \LL' &= e^m \frac{\partial \LL'}{\partial \dot{x}_{km}'}
\end{aligned}
\end{equation}

Then the chain rule application above becomes

\begin{equation}\label{eqn:goldsteinCh12:420}
\begin{aligned}
\frac{d \LL'}{d\alpha} (\Bx_k', \dot{\Bx}_j')
&= \sum_{k} \left( \spacegrad_{\Bx_k'} \LL'\right) \cdot \PD{\alpha}{\Bx_k'} + \left( \spacegrad_{\dot{\Bx}_k'} \LL' \right) \cdot \PD{\alpha}{\dot{\Bx}_k'} \\
\end{aligned}
\end{equation}

Now, while this notational sugar unfortunately has an obscuring effect, it is also practical since we can now work with the transformed position
and velocity vectors directly

\begin{equation}\label{eqn:goldsteinCh12:440}
\begin{aligned}
\PD{\alpha}{\Bx_k'}
&= (-i \ncap/2) e^{-i\ncap \alpha/2} {\Bx}_k e^{i\ncap\alpha/2} +e^{-i\ncap \alpha/2} {\Bx}_k e^{i\ncap\alpha/2} (i \ncap/2)  \\
&= (-i \ncap/2) \Bx_k' + \Bx_k' (i \ncap/2) \\
&= i (\ncap \wedge \Bx_k')
\end{aligned}
\end{equation}

So we have
\begin{equation}\label{eqn:goldsteinCh12:460}
\begin{aligned}
\frac{d \LL'}{d\alpha} (\Bx_k', \dot{\Bx}_j')
&= \sum_{k} \left( \spacegrad_{\Bx_k'} \LL'\right) \cdot \left( i (\ncap \wedge \Bx_k') \right)
+ \sum_{k} \left( \spacegrad_{\dot{\Bx}_k'} \LL'\right) \cdot \left( i (\ncap \wedge \dot{\Bx}_k') \right) \\
\end{aligned}
\end{equation}

Next step is to reintroduce the notational sugar noting that we can vectorize the Euler-Lagrange equations by writing

\begin{equation}\label{eqn:goldsteinCh12:480}
\begin{aligned}
\spacegrad_{\Bx_k} \LL = \frac{d}{dt} \spacegrad_{\dot{\Bx}_k} \LL
\end{aligned}
\end{equation}

We have now a three fold reduction in the number of Euler-Lagrange equations.  For each of the generalized vector parameters, we have the
Lagrangian gradient with respect to that vector parameter (a generalized force) equals the time rate of change of the velocity gradient.

Inserting this we have
\begin{equation}\label{eqn:goldsteinCh12:500}
\begin{aligned}
\frac{d \LL'}{d\alpha} (\Bx_k', \dot{\Bx}_j')
&= \sum_{k} \left( \frac{d}{dt} \spacegrad_{\dot{\Bx}_k'} \LL'\right) \cdot \left( i (\ncap \wedge \Bx_k') \right)
+ \sum_{k} \left( \spacegrad_{\dot{\Bx}_k'} \LL'\right) \cdot \left( i (\ncap \wedge \dot{\Bx}_k') \right) \\
\end{aligned}
\end{equation}

Now we can drop the primes in gradient terms because of the Lagrangian invariance for this symmetry, and are left almost with a
perfect differential

\begin{equation}\label{eqn:goldsteinCh12:520}
\begin{aligned}
\frac{d \LL'}{d\alpha} (\Bx_k', \dot{\Bx}_j')
&= \sum_{k} \left( \frac{d}{dt} \spacegrad_{\dot{\Bx}_k} \LL\right) \cdot \left( i (\ncap \wedge \Bx_k') \right)
+ \sum_{k} \left( \spacegrad_{\dot{\Bx}_k} \LL\right) \cdot \left( i (\ncap \wedge \dot{\Bx}_k') \right) \\
\end{aligned}
\end{equation}

Here is where the evaluation at \(\alpha=0\) comes in, since \(\Bx_k'(\alpha=0) = \Bx_k\), and we can now antidifferentiate

\begin{equation}\label{eqn:goldsteinCh12:540}
\begin{aligned}
{\left. \frac{d \LL'}{d\alpha} (\Bx_k', \dot{\Bx}_j') \right\vert}_{\alpha=0}
&= \sum_{k} \left( \frac{d}{dt} \spacegrad_{\dot{\Bx}_k} \LL\right) \cdot \left( i (\ncap \wedge \Bx_k) \right)
+ \sum_{k} \left( \spacegrad_{\dot{\Bx}_k} \LL\right) \cdot \left( i (\ncap \wedge \dot{\Bx}_k) \right) \\
&= \sum_{k} \frac{d}{dt} \left( \left( \spacegrad_{\dot{\Bx}_k} \LL\right) \cdot \left( i (\ncap \wedge \Bx_k) \right) \right) \\
&= \sum_{k} \frac{d}{dt} \gpgradezero{ \left( \spacegrad_{\dot{\Bx}_k} \LL \right) i (\ncap \wedge \Bx_k) } \\
&= \sum_{k} \frac{d}{dt} \inv{2} \gpgradezero{ \left( \spacegrad_{\dot{\Bx}_k} \LL \right) i (\ncap \Bx_k - \Bx_k \ncap) } \\
&= \sum_{k} \frac{d}{dt} \inv{2} \gpgradezero{
\ncap i \left( \Bx_k \left( \spacegrad_{\dot{\Bx}_k} \LL \right) -  \left( \spacegrad_{\dot{\Bx}_k} \LL \right) \Bx_k \right)
} \\
&= \sum_{k} \frac{d}{dt} \inv{2} \gpgradezero{
\ncap i \left( \Bx_k \left( \spacegrad_{\dot{\Bx}_k} \LL \right) -  \left( \spacegrad_{\dot{\Bx}_k} \LL \right) \Bx_k \right)
} \\
&= \sum_{k} \frac{d}{dt} \gpgradezero{
\ncap i \left(\Bx_k \wedge \left( \spacegrad_{\dot{\Bx}_k} \LL \right) \right)
} \\
&= \sum_{k} \frac{d}{dt} \gpgradezero{
\ncap i^2 \left(\Bx_k \cross \left( \spacegrad_{\dot{\Bx}_k} \LL \right) \right)
} \\
&= \sum_{k} \frac{d}{dt} -\ncap \cdot \left(\Bx_k \cross \left( \spacegrad_{\dot{\Bx}_k} \LL \right) \right) \\
\end{aligned}
\end{equation}

Because of the symmetry this entire derivative is zero, so we have

\begin{equation}\label{eqn:goldsteinCh12:560}
\begin{aligned}
\ncap \cdot \sum_{k} \left(\Bx_k \cross \left( \spacegrad_{\dot{\Bx}_k} \LL \right) \right) &= \text{constant}
\end{aligned}
\end{equation}

The Lagrangian velocity gradient can be identified as the momentum
(ie: the canonical momentum conjugate to \(\Bx_k\))

\begin{equation}\label{eqn:goldsteinCh12:580}
\begin{aligned}
\Bp_k &\equiv \spacegrad_{\dot{\Bx}_k} \LL
\end{aligned}
\end{equation}

Also noting that this is constant for any \(\ncap\), we finally have the
conserved ``current'' for a rotational symmetry of a system of particles

\begin{equation}\label{eqn:goldsteinCh12:600}
\begin{aligned}
\sum_{k} \Bx_k \cross \Bp_k &= \text{constant}
\end{aligned}
\end{equation}

This digression to Noether's appears to be well worth it for answering the angular momentum question of the problem.  Glibly saying ``yes
angular momentum is conserved'', just because the Lagrangian has a rotational symmetry is not enough.  We have seen in this particular
problem that the Lagrangian, having only dot products has the rotational symmetry, but because of the velocity dependent
potential terms \(f^i \dot{\Brho}_k \cdot \dot{\Brho}_j\), the normal Kinetic energy momentum vectors are not equal to the canonical conjugate
momentum vectors.  Only when the angular momentum is generalized, and written in terms of the canonical conjugate momentum is the total
system angular momentum conserved.  Namely, the generalized angular momentum for this problem is conserved

\begin{equation}\label{eqn:goldsteinCh12:620}
\begin{aligned}
\sum_{k} \Bx_k \cross \left( \spacegrad_{\dot{\Bx}_k} \LL \right) &= \text{constant}
\end{aligned}
\end{equation}

but the ``traditional'' angular momentum \(\sum_k \Bx_k \cross m \dot{\Bx}_k\), is not.
}

\makeproblem{Shortest curve variational problem (2.1)}{gold:ch2:pr1}{
Prove that the shortest length curve between two points in space is a straight line.
}

\makeanswer{gold:ch2:pr1}{
A first attempt of this I used:

\begin{equation*}
ds = \sqrt{ 1 + (dy/dx)^2 + (dz/dx)^2 } dx
\end{equation*}

Application of the Euler-Lagrange equations does show that one ends up with a linear relation between the y and z coordinates, but no mention of x.  Rather than write that up, consider instead a parametrization of the coordinates:

\begin{equation}\label{eqn:goldsteinCh12:640}
\begin{aligned}
x &= x_1(\lambda) \\
y &= x_2(\lambda) \\
z &= x_3(\lambda)
\end{aligned}
\end{equation}

in terms of this arbitrary parametrization we have a segment length of:

\begin{equation*}
ds = \sqrt{ \sum \left(\frac{d x_i}{d\lambda}\right)^2 } d \lambda = f\left(x_i\right) d\lambda
\end{equation*}

Application of the Euler-Lagrange equation to \(f\) we have:

\begin{equation}\label{eqn:goldsteinCh12:660}
\begin{aligned}
\PD{x_i}{f}
&= 0 \\
&= \frac{d}{d\lambda} \PD{\xdot_i}{} \sqrt{ \sum {\xdot_j}^2 } \\
&= \frac{d}{d\lambda} \frac{ \xdot_i }{\sqrt{ \sum {\xdot_j}^2 }}
\end{aligned}
\end{equation}

Therefore each of these quotients can be equated to a constant:

\begin{equation}\label{eqn:goldsteinCh12:680}
\begin{aligned}
\frac{ \xdot_i }{\sqrt{ \sum {\xdot_j}^2 }} &= {c_i}^{-2} \\
{c_i}^2 \xdot_i^2 &= \sum {\xdot_j}^2 \\
({c_i}^2 -1)\xdot_i^2 &= \sum_{j \ne i} {\xdot_j}^2 \\
(1 - {c_i}^2)\xdot_i^2 + \sum_{j \ne i} {\xdot_j}^2 &= 0
\end{aligned}
\end{equation}

This last form shows explicitly that not all of these squared derivative terms can be linearly independent.  In particular, we have a
zero determinant:

\begin{equation*}
0 =
\begin{vmatrix}
1 - c_1^2   & 1            & 1         & 1 & \hdots \\
1           & 1 - c_2^2    & 1         & 1 & \vdots \\
1           & 1            & 1 - c_3^2 & 1 & \\
            &              &           & \ddots & \\
            &              &           &        & 1 - {c_n}^2
\end{vmatrix}
\end{equation*}

Now, expanding this for a couple specific cases is not too hard.  For \(n=2\) we have:

\begin{equation}\label{eqn:goldsteinCh12:700}
\begin{aligned}
0 &= (1 - c_1^2)(1-c_2^2) - 1 \\
c_1^2 + c_2^2 &= c_1^2 c_2^2 \\
c_1^2 &= \frac{c_2^2}{ c_2^2 - 1 } \\
c_2^2 - 1 &= \frac{c_2^2}{ c_1^2 }
\end{aligned}
\end{equation}

This can be substituted back into one our \(c_2^2\) equation:

\begin{equation}\label{eqn:goldsteinCh12:720}
\begin{aligned}
({c_2}^2 -1)\xdot_2^2 &= {\xdot_1}^2 \\
\frac{c_2^2}{ c_1^2 } \xdot_2^2 &= {\xdot_1}^2 \\
\pm \frac{c_2}{ c_1 } \xdot_2 &= {\xdot_1} \\
\pm \frac{c_2}{ c_1 } x_2 &= x_1 + \kappa \\
\end{aligned}
\end{equation}

This is precisely the straight line that was desired, but we have setup for proving that consideration of all path variations from two points
in \R{N} space has the shortest distance when that path is a straight line.

Despite the general setup, I am going to chicken out and show this only for the \R{3} case.  In that case our determinant expands to:

\begin{equation*}
c_1^2 + c_2^2 + c_3^2 = c_1^2 c_2^2 c_3^2
\end{equation*}

Since not all of the \(\xdot_i^2\) can be linearly independent, one can be eliminated:

\begin{equation}\label{eqn:goldsteinCh12:740}
\begin{aligned}
(1 - c_1^2) \xdot_1^2 + \xdot_2^2 + \xdot_3^2 &= 0 \\
(1 - c_2^2) \xdot_2^2 + \xdot_3^2 + \xdot_1^2 &= 0 \\
(1 - c_3^2) \xdot_3^2 + \xdot_1^2 + \xdot_2^2 &= 0
\end{aligned}
\end{equation}

Let us pick \(\xdot_1^2\) to eliminate, and subst 2 into 3:

\begin{equation}\label{eqn:goldsteinCh12:760}
\begin{aligned}
%(1 - c_1^2) (-(1 - c_2^2) \xdot_2^2 - \xdot_3^2) + \xdot_2^2 + \xdot_3^2 &= 0 \\
(1 - c_3^2) \xdot_3^2 + (-(1 - c_2^2) \xdot_2^2 - \xdot_3^2) + \xdot_2^2 &= 0
\implies \\
%\xdot_2^2 ( 1 - (1 - c_1^2)(1 - c_2^2) ) + \xdot_3^2 ( 1 - (1 - c_1^2) ) &= 0 \\
- c_3^2 \xdot_3^2 + c_2^2 \xdot_2 &= 0 \\
\pm c_3 \xdot_3 &= c_2 \xdot_2 \\
\end{aligned}
\end{equation}

%Which is, once again a straight line:
%
%\begin{equation*}
%\pm c_3 x_3 = c_2 x_2 + \kappa
%\end{equation*}

Since these equations are symmetric, we can do this for all, with the result:
\begin{equation}\label{eqn:goldsteinCh12:780}
\begin{aligned}
\pm c_3 \xdot_3 &= c_2 \xdot_2 \\
\pm c_3 \xdot_3 &= c_1 \xdot_1 \\
\pm c_2 \xdot_2 &= c_1 \xdot_1 \\
\end{aligned}
\end{equation}

Since the \(c_i\) constants are arbitrary, then we can for example pick the negative sign for \(\pm c_2\), and the positive for the rest, then add all of these, and scale by two:

\begin{equation*}
c_3 \xdot_3 - c_2 \xdot_2 = c_1 \xdot_1
\end{equation*}

and integrating:

\begin{equation*}
c_3 x_3 - c_2 x_2 = c_1 x_1 + \kappa
\end{equation*}

Again, we have the general equation of a line, subject to the desired constraints on the end points.  In the end we did not need to
evaluate the determinant after all, as done in the
\R{2} case.
}

\makeproblem{Geodesics on sphere (2.2)}{gold:ch2:pr2}{
Prove that the geodesics (shortest length paths) on a spherical surface are great circles.
}
\makeanswer{gold:ch2:pr2}{

As a variational problem, the first step is to formulate an element of length on the surface.  If we write our vector in spherical coordinates (\(\phi\) on the equator, and \(\theta\) measuring from the north pole) we have:

FIXME: Scan picture.

\begin{equation*}
\Br = (x, y, z) = R( \sin\theta cos\phi, \sin\theta \sin\phi, \cos\theta)
\end{equation*}

A differential vector element on the surface is (set \(R=1\) without loss of generality) :

\begin{equation}\label{eqn:goldsteinCh12:800}
\begin{aligned}
d \Br
&= \frac{d\Br}{d \theta} \frac{d \theta}{d \lambda} d \lambda + \frac{d\Br}{d \phi} \frac{d \phi}{d \lambda} d \lambda \\
&=
 ( \cos\theta \cos\phi, \cos\theta \sin\phi, -\sin\theta) \dottheta d\lambda
+( -\sin\theta \sin\phi, \sin\theta \cos\phi, 0) \dotphi d\lambda \\
&=
 ( \cos\theta \cos\phi \dottheta - \sin\theta \sin\phi \dotphi,
   \cos\theta \sin\phi \dottheta + \sin\theta \cos\phi \dotphi,
  -\sin\theta \dottheta) d\lambda
\end{aligned}
\end{equation}

Calculation of the length \(ds\) of this vector yields:

\begin{equation*}
ds = \Abs{ d\Br} = \sqrt{\dottheta^2 + (\sin\theta)^2 \dotphi^2} d\lambda
\end{equation*}

This completes the setup for the minimization problem, and we want to
minimize:

\begin{equation*}
s = \int \sqrt{\dottheta^2 + ( \dotphi \sin\theta )^2 } d\lambda
\end{equation*}

and can therefore apply the Euler-Lagrange equations to the function

\begin{equation*}
f(\theta, \phi, \dottheta, \dotphi, \lambda) =
\sqrt{\dottheta^2 + ( \dotphi \sin\theta )^2 }
\end{equation*}

The \(\phi\) is cyclic, and we have:

\begin{equation*}
\PD{\phi}{f} = 0 = \frac{d}{d\lambda} \frac{\dotphi \sin^2\theta}{f}
\end{equation*}

Thus we have:
\begin{equation}\label{eqn:goldsteinCh12:820}
\begin{aligned}
\dotphi^2 \sin^4\theta &= K^2 \left(\dottheta^2 +
\lr{  \dotphi \sin\theta  }^2
 \right) \\
\dotphi^2 \sin^2\theta
\lr{  \sin^2\theta - K^2  }
 &= K^2 \dottheta^2 \\
\dotphi^2
&= \frac{K^2 \dottheta^2 }{ \sin^2\theta
\lr{  \sin^2\theta - K^2  } } \\
\dotphi
&= \frac{K \dottheta }{ \sin\theta \sqrt{ \sin^2\theta - K^2 } } \\
\end{aligned}
\end{equation}

This is in a nicely separated form, but it is not obvious that this describes paths that are great circles.

Let us have a look at the second equation.
\begin{equation}\label{eqn:goldsteinCh12:840}
\begin{aligned}
\PD{\theta}{f} &= \frac{d}{d\lambda} \PD{\dottheta}{f} \\
\frac{\sin\theta\cos\theta \dotphi^2}{f}
&= \frac{d}{d\lambda} \frac{\dottheta}{f} \\
&= \frac{\ddottheta}{f} - \inv{2} \frac{
\lr{ \dottheta^2 + \lr{  \dotphi \sin\theta  }^2 }'
 }{f^3} \\
&= \frac{\ddottheta}{f} - \frac{ \dottheta \ddottheta + \dotphi \sin\theta
\lr{  \ddotphi \sin\theta + \dotphi \cos\theta \dottheta  }
}{f^3} \\
\implies
-\sin\theta\cos\theta \dotphi^2
\lr{  \dottheta^2 + \lr{  \dotphi \sin\theta  }^2  }
&= -\ddottheta
\lr{  \dottheta^2 + \lr{  \dotphi \sin\theta  }^2  }
   + \dottheta \ddottheta
   + \dotphi \sin\theta
\lr{  \ddotphi \sin\theta + \dotphi \cos\theta \dottheta  } \\
\end{aligned}
\end{equation}

Or,
\begin{equation*}
- \ddottheta \dottheta^2
- \ddottheta \dotphi^2 \sin^2\theta
+ \dottheta \ddottheta
+ \dotphi \ddotphi \sin^2\theta
+ \dotphi^2 \dottheta \sin\theta \cos\theta
+ \dotphi^2 \dottheta^2 \sin\theta \cos\theta
+ \dotphi^4 \sin^3\theta \cos\theta
= 0
\end{equation*}

What a mess!  I do not feel inclined to try to reduce this at the moment.  I will come back to this problem later.  Perhaps there is a better parametrization?

Did come back to this later, in \citep{miscphysics:PJbyronFullerCalcVarProblems}, but
still did not get the problem fully solved.  Maybe the third time, some time
later, will be the charm.
}

\makeproblem{Euler Lagrange equations for second order systems (2.4)}{gold:ch2:pr3}{
For \(f = f( y, \ydot, \yddot, x )\), find the equations for extreme values of

\begin{equation*}
I = \int_a^b f dx
\end{equation*}
}

\makeanswer{gold:ch2:pr3}{

Here we want \(y\) and \(\ydot\) fixed at the end points.  Following the first derivative derivation write the
functions in terms of the desired extremum functions plus a set of arbitrary functions:

\begin{equation}\label{eqn:goldsteinCh12:860}
\begin{aligned}
y( x, \alpha ) &= y( x, 0 ) + \alpha n(x) \\
\ydot( x, \alpha ) &= \ydot( x, 0 ) + \alpha m(x)
\end{aligned}
\end{equation}

Here we specify that these arbitrary variational functions vanish at the endpoints:

\begin{equation*}
n(a) = n(b) = m(a) = m(b) = 0
\end{equation*}

The functions \(y(x, 0)\), and \(\ydot(x, 0)\) are the functions we are looking for as solutions to the min/max problem.

Calculating derivatives we have:

\begin{equation*}
\frac{dI}{d\alpha} =
\int \left(
\PD{y}{f} \PD{\alpha}{y}
+\PD{\ydot}{f} \PD{\alpha}{\ydot}
+\PD{\yddot}{f} \PD{\alpha}{\yddot}
\right) d x
\end{equation*}

Assuming sufficient continuity look at the second term where we have:

\begin{equation}\label{eqn:goldsteinCh12:880}
\begin{aligned}
\PD{\alpha}{\ydot}
&= \PD{\alpha}{} \PD{x}{y} \\
&= \PD{x}{} \PD{\alpha}{y} \\
&= \PD{x}{} n(x) \\
&= \frac{d}{ d x} n(x) \\
&= \frac{d}{ d x} \PD{\alpha}{y} \\
\end{aligned}
\end{equation}

Similarly for the third term we have:

\begin{equation*}
\PD{\alpha}{\ydot} = \frac{d}{ d x} \PD{\alpha}{\ydot}
\end{equation*}

\begin{equation*}
\frac{dI}{d\alpha} =
\int \PD{y}{f} \PD{\alpha}{y} d x +
\mathLabelBox{\PD{\ydot}{f} \frac{d}{ d x} \PD{\alpha}{y}}{\(u v' = (u v)' - u' v \)}
d x
+\PD{\yddot}{f} \frac{d}{ d x} \PD{\alpha}{\ydot} d x
\end{equation*}

Now integrating by parts:
\begin{equation}\label{eqn:goldsteinCh12:900}
\begin{aligned}
\frac{dI}{d\alpha} &=
 \int \PD{y}{f} \PD{\alpha}{y} d x
+\int \PD{\ydot}{f} \frac{d}{ d x} \PD{\alpha}{y} d x
+\int \PD{\yddot}{f} \frac{d}{ d x} \PD{\alpha}{\ydot} d x \\
\frac{dI}{d\alpha} &=
 \int \PD{y}{f} \PD{\alpha}{y} d x
+\Bigl(
\PD{\ydot}{f}
\mathLabelBox{\PD{\alpha}{y}}{\(n(x)\)}
\Bigr)_a^b - \int \PD{\alpha}{y} \frac{d}{ d x} \PD{\ydot}{f} d x
+\Bigl(
\PD{\yddot}{f}
\mathLabelBox{\PD{\alpha}{\ydot}}{\(m(x)\)}
\Bigr)_a^b
-\int \PD{\alpha}{\ydot} \frac{d}{ d x} \PD{\yddot}{f} d x
\end{aligned}
\end{equation}

Because \(m(a) = m(b) = n(a) = n(b)\) the non-integral terms are all zero, leaving:

\begin{equation}\label{eqn:goldsteinCh12:920}
\begin{aligned}
\frac{dI}{d\alpha} &=
  \int \PD{y}{f} \PD{\alpha}{y} d x
- \int \PD{\alpha}{y} \frac{d}{ d x} \PD{\ydot}{f} d x
- \int \PD{\alpha}{\ydot} \frac{d}{ d x} \PD{\yddot}{f} d x
\end{aligned}
\end{equation}

Now consider just this last integral, which we can again integrate by parts:
\begin{equation}\label{eqn:goldsteinCh12:940}
\begin{aligned}
\int \PD{\alpha}{\ydot} \frac{d}{ d x} \PD{\yddot}{f} d x
&= \int
\mathLabelBox{\frac{d}{dx} \PD{\alpha}{y}}{\(u'\)}
\mathLabelBox{\frac{d}{ d x} \PD{\yddot}{f}}{\(v\)}
d x \\
&=
\Bigl(
\mathLabelBox{\PD{\alpha}{y}}{\(n(x)\)}
{\frac{d}{ d x} \PD{\yddot}{f}}
\Bigr)_a^b
-\int \PD{\alpha}{y} \frac{d}{dx} {\frac{d}{ d x} \PD{\yddot}{f}} d x \\
&=
-\int \PD{\alpha}{y} \frac{d^2}{dx^2} \PD{\yddot}{f} d x \\
\end{aligned}
\end{equation}

This gives:
\begin{equation}\label{eqn:goldsteinCh12:960}
\begin{aligned}
\frac{dI}{d\alpha} &=
  \int \PD{y}{f} \PD{\alpha}{y} d x
- \int \PD{\alpha}{y} \frac{d}{ d x} \PD{\ydot}{f} d x
+ \int \PD{\alpha}{y} \frac{d^2}{dx^2} \PD{\yddot}{f} d x \\
\frac{dI}{d\alpha}
&= \int d x \PD{\alpha}{y} \left( \PD{y}{f} - \frac{d}{ d x} \PD{\ydot}{f} + \frac{d^2}{dx^2} \PD{\yddot}{f} \right) \\
&= \int d x n(x) \left( \PD{y}{f} - \frac{d}{ d x} \PD{\ydot}{f} + \frac{d^2}{dx^2} \PD{\yddot}{f} \right)
\end{aligned}
\end{equation}

So, if we want this derivative to equal zero for any \(n(x)\) we require the inner quantity to by zero:

\begin{equation}
\PD{y}{f} - \frac{d}{ d x} \PD{\ydot}{f} + \frac{d^2}{dx^2} \PD{\yddot}{f} = 0
\end{equation}

Question.  Goldstein writes this in total differential form instead of a derivative:

\begin{equation}\label{eqn:goldsteinCh12:980}
\begin{aligned}
dI &= \frac{dI}{d\alpha} d\alpha \\
&= \int d x \left(\PD{\alpha}{y} d \alpha\right) \left( \PD{y}{f} - \frac{d}{ d x} \PD{\ydot}{f} + \frac{d^2}{dx^2} \PD{\yddot}{f} \right) \\
\end{aligned}
\end{equation}

and then calls this quantity \(\PD{\alpha}{y} d \alpha = \delta y\), the variation of \(y\).  There must be a mathematical subtlety which motivates this
but it is not clear to me what that is.  Since the variational calculus texts go a different route, with norms on functional spaces and so forth, perhaps
understanding that motivation is not worthwhile.  In the end, the conclusion is the same, namely that the inner expression must equal zero for the extremum
condition.
}
